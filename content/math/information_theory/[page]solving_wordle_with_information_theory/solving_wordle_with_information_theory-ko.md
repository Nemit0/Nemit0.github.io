---
title: "정보이론으로 Wordle 풀기"
description: "샤넌 엔트로피와 정보 최대화를 활용한 인기 단어 게임의 수학적 접근법."
date: "2025-12-21"
category: "math/information_theory"
tags: ["수학", "정보이론", "알고리즘", "wordle", "엔트로피"]
author: "Nemit"
---

단어 맞히기 게임 Wordle, 아시죠? 몇 년 전 전 세계를 강타했던 그 게임 말입니다. 그런데 혹시 *최적의* 전략이 무엇인지 궁금해 본 적 있으신가요? 단순히 좋은 시작 단어를 고르는 것이 아니라, 수학적으로 증명된 가장 효율적인 풀이법 말입니다.

그 답은 **정보이론**에 있습니다.

## 문제 공간

Wordle은 숨겨진 5글자 영단어를 여섯 번의 시도 안에 맞히는 게임입니다.
*   **초록색:** 올바른 글자, 올바른 위치.
*   **노란색:** 올바른 글자, 잘못된 위치.
*   **회색:** 단어에 없는 글자.

원본 Wordle 사전에는 약 **2,315개**의 정답 후보 단어가 있습니다 (정답이 되는 경우가 드문 추가 허용 추측 단어는 약 10,000개가 더 있습니다).

단순히 무작위로 추측한다면, 정답 확률은 $\frac{1}{2315}$에 불과합니다. 이를 체계적으로 풀려면, 가능한 정답 후보를 최대한 빠르게 줄여나가야 합니다.

## 불확실성의 정량화: 엔트로피

Wordle에 뛰어들기 전에, **정보이론**의 기본 개념을 먼저 정립합시다.

정보의 핵심은 **놀라움**입니다.
*   "내일 해가 뜰 것이다"라고 말하면, 거의 아무런 정보도 얻지 못합니다. 이미 알고 있는 사실이니까요.
*   "내일 사하라 사막에 눈이 올 것이다"라고 말하면, 그것은 매우 정보량이 높습니다. 드물고 예상 밖이기 때문입니다.

### 비트: 놀라움의 단위
정보는 **비트(bit)** 단위로 측정합니다. 1비트는 공정한 "예/아니오" 질문 (두 답이 동일한 확률인 경우)에 대한 답에서 얻는 정보량입니다.

내가 1부터 8 사이의 숫자 하나를 골랐다고 상상해 봅시다.
*   **질문 1:** 4보다 큰가요? (절반 제거) → 4개 남음.
*   **질문 2:** 짝수인가요? (절반 제거) → 2개 남음.
*   **질문 3:** 6인가요? (절반 제거) → 1개 남음.

답을 찾는 데 **3비트**의 정보 ($\log_2 8 = 3$)가 필요했습니다.

### Wordle에 적용하기
Wordle에는 약 **2,315개**의 가능한 정답이 있습니다.
모든 단어가 동일한 확률 ($p = \frac{1}{2315}$)이라면, 초기 불확실성(엔트로피)은 다음과 같습니다:

$$ 
H = -\log_2 \left( \frac{1}{2315} \right) \approx 11.17 \text{ bits} 
$$

이는 후보를 하나의 단어로 좁히기 위해 약 11.17비트의 정보를 "획득"해야 한다는 의미입니다.
*   후보를 절반으로 줄이는 추측은 **1비트**를 제공합니다.
*   후보를 1/4로 줄이는 추측은 **2비트**를 제공합니다.
*   최적의 Wordle 솔버의 목표는 평균적으로 **최대 비트 수**를 제공하는 추측을 선택하는 것입니다.

## 피드백 루프

추측을 입력하면 게임은 색상 패턴을 반환합니다. 초록, 노랑, 회색의 가능한 패턴은 $3^5 = 243$가지입니다.

*   **정보량이 높은 추측**은 가능한 정답들이 다양한 패턴에 고르게 분포되는 것입니다. 게임이 어떤 패턴을 반환하든, 남은 단어 목록이 크게 줄어듭니다.
*   **좋지 않은 추측**은 대부분의 가능한 정답이 같은 패턴(예: 전부 회색)을 만드는 것입니다. 그 흔한 패턴을 받으면, 배운 것이 별로 없어 여전히 많은 후보가 남게 됩니다.

## 전략: 기대 정보량 최대화

최적으로 플레이하려면, 바로 정답을 맞히려 하기보다는 (남은 후보가 하나뿐일 때는 예외) **기대 정보 이득 (Expected Information Gain, EIG)**을 최대화하는 단어를 골라야 합니다.

주어진 추측 단어 $w$에 대해, 남은 후보들의 기대 엔트로피는 가능한 모든 색상 패턴 $p$에 대해 합산하여 계산합니다:

$$ E[w] = \sum_{p} P(p) \times (-\log_2(P(p))) 
$$ 

여기서 $P(p)$는 현재 가능한 단어 목록에서 패턴 $p$를 받을 확률입니다.

### 알고리즘
1.  현재 유효한 정답 후보 목록을 확인합니다.
2.  가능한 모든 추측 (정답이 *될 수 없는* 단어 포함)에 대해, 모든 정답 후보에 대해 시뮬레이션합니다.
3.  결과로 나오는 색상 패턴의 분포를 계산합니다.
4.  해당 분포의 엔트로피를 계산합니다.
5.  **엔트로피가 가장 높은 단어를 선택합니다.**

## 언어 이론과 빈도 분석

최적의 시작 단어를 찾으려면, 먼저 우리가 다루는 데이터, 즉 영어를 이해해야 합니다.

모든 알파벳이 동등하게 만들어진 것은 아닙니다. 일반적인 영어 텍스트에서 가장 빈번한 글자는 대략 **E, T, A, O, I, N, S, H, R, D, L, U**입니다. 그러나 Wordle은 5글자 단어의 특정 부분집합을 사용합니다.

### "Wordle" 빈도
Wordle 정답 목록 (2,315 단어)에서 빈도 분포는 약간 달라집니다.
1.  **E**는 여전히 왕입니다 (단어의 약 46%에 등장).
2.  **A** (~43%)와 **R** (~40%)이 매우 흔합니다.
3.  **S**는 까다롭습니다. 영어에서 매우 흔하지만, 5글자 "S" 단어의 상당수가 복수형이며, Wordle의 원본 정답 목록에서는 대부분 제외됩니다. 그러나 'S'는 여전히 확인해야 할 중요한 글자입니다.

하지만 빈도만으로는 부족합니다. **위치별 빈도**가 필요합니다.
*   **S**는 정답 단어의 *끝*에는 드물지만 *처음*에는 흔합니다.
*   **Y**는 *끝*에 매우 흔하지만 처음에는 드뭅니다.
*   **E**는 *끝*과 *중간*에 매우 흔합니다.

순진한 전략은 "E, A, R, O, T"가 포함된 단어를 고르는 것일 수 있습니다. 하지만 정보이론은 더 깊은 질문을 던집니다: **어떤 단어가 후보를 가장 잘 분할하는가?**

## 판결: 최적의 시작 단어

전체 사전에 대해 시뮬레이션을 실행하고 모든 가능한 시작 단어의 기대 정보 이득을 계산하면, 몇 가지 강력한 후보가 떠오릅니다.

### 1. "SOARE" / "ROATE" / "RAISE"
이들은 불확실성을 줄이는 데 있어서 수학적으로 "최고"의 시작 단어로 자주 꼽힙니다.
*   **엔트로피:** ~5.87비트.
*   **이유:** 가장 빈번한 글자(R, A, E, S, T, O)를 확률이 높은 위치에 사용합니다.
*   **결과:** "SOARE"를 입력한 후, 평균 남은 정답 후보 수는 2,315개에서 약 **60개**로 줄어듭니다.

### 2. "CRANE" / "SALET"
*   **CRANE**은 3Blue1Brown의 첫 번째 봇 버전에서 최상위로 식별된 단어입니다.
*   **SALET**은 "하드 모드"를 플레이하는 봇들에게 자주 선호되는데, 두 번째 추측에 유연한 선택지를 남겨주기 때문입니다.

### 3. "최악"의 시작 단어들
반대로, **"FUZZY"**, **"XYLYL"**, **"JAZZY"** 같은 단어는 엔트로피가 매우 낮습니다.
*   정답이 "FUZZY"가 아니라면, 5개 모두 회색을 받을 가능성이 높습니다.
*   "FUZZY"에서 5개 회색을 받으면, 단어에 F, U, Z, Y가 없다는 것만 알게 됩니다. 이는 *수천 개*의 단어가 여전히 남아 있다는 뜻입니다. 거의 아무것도 배우지 못한 셈입니다.

## 정보의 함정: 실전 예시

정보가 초록색 글자를 맞히는 것보다 왜 중요한지 시각화하기 위해, 단어가 **-ATCH**로 끝난다는 것을 알게 된 상황을 상상해 봅시다.

남은 후보: *BATCH, CATCH, HATCH, LATCH, MATCH, PATCH, WATCH*.

"하드 모드"로 플레이하거나 순진하게 추측한다면, **BATCH**를 추측할 수 있습니다.
*   맞으면: 좋습니다! (1/7 확률)
*   틀리면: ⬜🟩🟩🟩🟩를 받습니다. 여전히 6개 남았습니다.
*   그다음 **CATCH**를 추측합니다. 틀리면? 5개 남음.
*   정답을 찾기 전에 시도 횟수가 쉽게 바닥날 수 있습니다.

**정보이론적 접근:**
정답이 될 수 있는 단어를 추측하는 대신, **"CLAMP"** 같은 "소모" 단어를 입력합니다.
*   이 단어는 **C**, **L**, **M**, **P**를 사용합니다.
*   정답이 될 수는 없지만 (-ATCH로 끝나지 않으므로), 탐색 공간을 극적으로 줄여줍니다.
    *   **C**가 표시되면: 정답은 *CATCH*.
    *   **L**이 표시되면: 정답은 *LATCH*.
    *   **M**이 표시되면: 정답은 *MATCH*.
    *   **P**가 표시되면: 정답은 *PATCH*.
    *   아무것도 표시되지 않으면: *BATCH, HATCH,* 또는 *WATCH*.

즉시 이길 확률이 0%인 단어에 한 턴을 투자함으로써, 최대 정보를 획득하여 다음 턴에 확실히 이길 수 있게 됩니다.

## 하드 모드 vs. 노멀 모드

게임 모드에는 미묘하지만 중요한 차이가 있습니다.

*   **노멀 모드:** 이전 단서에 따르면 정답이 될 수 없는 단어라 해도, 순전히 정보를 얻기 위한 "소모" 추측을 할 수 있습니다. 이것이 수학적으로 최적인 경우가 많습니다. 예를 들어, *hatch, latch, catch, match* 사이에서 고민 중이라면 "CLAMP" 같은 단어를 입력하여 한 번에 C, L, M을 구별할 수 있습니다.
*   **하드 모드:** 찾은 단서를 반드시 사용해야 합니다 (예: 초록색 글자는 반드시 해당 위치에 사용). 이는 탐색 공간을 탐구하는 능력을 제한하며, 최적의 정보 수집 추측을 방해하여 "운에 맡기는" 상황으로 몰릴 수 있습니다.

## 결론

정보이론의 관점에서, Wordle은 단순한 단어 게임이 아닙니다. 이것은 **미니맥스(minimax)** 문제입니다. 매 단계에서 탐색 공간의 비트 감소를 최대화함으로써, 필요한 최대 추측 횟수를 최소화하는 것입니다.

모든 추측을 우주에 대한 질의로, 모든 색칠된 타일을 고유한 정보 패킷으로 취급하면, 수학적 정밀성으로 퍼즐을 풀 수 있습니다.

*참고: 이 접근법은 Grant Sanderson (3Blue1Brown)이 유명하게 시각화했으며, 수학과 게임의 교차점에 관심 있는 분이라면 반드시 봐야 할 영상입니다.
